{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1781f30",
   "metadata": {},
   "source": [
    "## Project Setup\n",
    "\n",
    "If using miniconda run this Command Prompt command: ```conda create --name project_env python=3.12```\n",
    "\n",
    "Before using this program, install everything in the `requirements.txt` file with this Command Prompt command:\n",
    "\n",
    "```pip install requirements.txt```\n",
    "\n",
    "Download the training dataset here: https://iris.di.ubi.pt/ubipr.html (Use the 'original' version.)\n",
    "\n",
    "Unzip the dataset into a folder somewhere, and set the DATASET_PATH field in `paths.env` to that directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22353837",
   "metadata": {},
   "source": [
    "## Import Statements and Parameters\n",
    "\n",
    "**Run code in this section before any other section.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267cd2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing...\")\n",
    "from lib import *\n",
    "print(\"Imported required modules.\")\n",
    "\n",
    "IMG_SIZE = (105, 105)                       # All images are scaled to this size (width, height).\n",
    "IMG_WITH_CHANNELS_SIZE = (105, 105, 3)      # The full input shape of the images, including color channels (width, height, channels).\n",
    "BATCH_SIZE = 32                             # The number of image-pair samples processed in a \"batch\" between each backpropogation pass.\n",
    "EPOCHS = 4                                  # How many epochs through the entire training dataset.\n",
    "STEPS_PER_EPOCH = 50                        # Number of batches the training loop runs per epoch.\n",
    "\n",
    "# The number of samples in each epoch is about `STEPS_PER_EPOCH * BATCH_SIZE`.\n",
    "\n",
    "# reads filepath configuration from `paths.env` and stores it in instance variables\n",
    "env = EnvLoader(\"paths.env\")\n",
    "print(str(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823aedfc",
   "metadata": {},
   "source": [
    "Load data from the filesystem if it exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb265e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gallery\n",
    "gallery_dict = load_gallery_images(env.GALLERY_IMAGE_PATH)\n",
    "\n",
    "# Define the structure of the CNN. Will eventually contain trained values.\n",
    "base_cnn = create_base_cnn(IMG_WITH_CHANNELS_SIZE)\n",
    "\n",
    "# Load gallery embeddings if they already exist\n",
    "gallery_embeddings = load_gallery_embeddings(env.GALLERY_EMBEDDING_PATH)\n",
    "\n",
    "try:\n",
    "    siamese_model = load_siamese_model(env.MODEL_SAVE_PATH)\n",
    "\n",
    "    # Copy weights from the trained Siamese model\n",
    "    # In the Siamese model, the base CNN is the 3rd layer (index 2)\n",
    "    for base_layer, siam_layer in zip(base_cnn.layers, siamese_model.layers[2].layers):\n",
    "        base_layer.set_weights(siam_layer.get_weights())\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(\"Model was not loaded\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713db7ea",
   "metadata": {},
   "source": [
    "## Siamese Model Training\n",
    "\n",
    "**Only run code in this section if new training data has been added, otherwise skip.**\n",
    "\n",
    "Dataset Image Preparation: generates tensorflow dataset for training and saves it to a cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if processed images already exist\n",
    "if os.path.exists(env.PROCESSED_IMAGE_PATH):\n",
    "    # Load already processed images (took 12s)\n",
    "    print(\"Loading preprocessed images from cache...\")\n",
    "    images_dict = np.load(env.PROCESSED_IMAGE_PATH, allow_pickle=True).item()\n",
    "else:\n",
    "    # Process images for the first time (took 10m 57s)\n",
    "    print(\"Processing raw images...\")\n",
    "    images_dict = load_images_by_filename(env.RAW_IMAGE_PATH, IMG_SIZE)\n",
    "    \n",
    "    print(f\"Saving preprocessed images to {env.PROCESSED_IMAGE_PATH}...\")\n",
    "    np.save(env.PROCESSED_IMAGE_PATH, images_dict)\n",
    "\n",
    "train_ds = make_tf_dataset(images_dict, BATCH_SIZE, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d848506f",
   "metadata": {},
   "source": [
    "Sanity check to make sure almost every person has the correct image count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82890588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_counts():\n",
    "    images_per_person = [len(imgs) for _person, imgs in images_dict.items()]\n",
    "    bins = np.bincount(images_per_person)\n",
    "    indices = np.arange(0, bins.shape[0])\n",
    "    return np.transpose(np.array((indices, bins)))\n",
    "\n",
    "get_image_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ce4c8",
   "metadata": {},
   "source": [
    "Training: runs the siamese model's training algorithm. Can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab355fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,            # stop after 5 epochs with no improvement\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "siamese_model, base_cnn = create_siamese_model(input_shape=IMG_WITH_CHANNELS_SIZE)\n",
    "siamese_model.compile(loss=contrastive_loss,\n",
    "              optimizer=Adam(learning_rate=1e-4),\n",
    "              metrics=[siamese_accuracy])\n",
    "siamese_model.summary()\n",
    "\n",
    "siamese_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "for base_layer, siam_layer in zip(base_cnn.layers, siamese_model.layers[2].layers):\n",
    "    base_layer.set_weights(siam_layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this after training:\n",
    "save_siamese_model(siamese_model, str(env.MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e7c65",
   "metadata": {},
   "source": [
    "## Process Gallery Images\n",
    "\n",
    "**Only run code in this section if the gallery has been changed.**\n",
    "\n",
    "The gallery contains a subfolder for each person containing images of their eyes.\n",
    "\n",
    "Each person's identifier will be the same as the name of their subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ea8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_gallery_images(env.GALLERY_IMAGE_PATH, IMG_SIZE)\n",
    "\n",
    "gallery_embeddings = compute_gallery_embeddings(\n",
    "    base_cnn,\n",
    "    gallery_dict,\n",
    "    IMG_WITH_CHANNELS_SIZE,\n",
    "    env.GALLERY_EMBEDDING_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff515d",
   "metadata": {},
   "source": [
    "## Compare Eyes\n",
    "\n",
    "**Run this code to perform the identification algorithm.**\n",
    "\n",
    "Uses `identify_eye()` to compare `query_image` with images in the gallery and outputs the image with the highest similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity, confidence = identify_eye(\n",
    "    env.QUERY_IMAGE_PATH,\n",
    "    \"Query Image\",\n",
    "    base_cnn,\n",
    "    gallery_embeddings,\n",
    "    IMG_SIZE,\n",
    "    margin=1.0,\n",
    "    threshold=70.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aeaa9a",
   "metadata": {},
   "source": [
    "Uses `identify_eye()` to identify all images in the TestImages folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326caca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in os.listdir(env.TEST_IMAGE_PATH):\n",
    "    if not img_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "    \n",
    "    query_image = os.path.join(env.TEST_IMAGE_PATH, img_name)\n",
    "\n",
    "    identity, confidence = identify_eye(\n",
    "        query_image,\n",
    "        img_name,\n",
    "        base_cnn,\n",
    "        gallery_embeddings,\n",
    "        img_size=IMG_SIZE,\n",
    "        margin=1.0,\n",
    "        threshold=70.0\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introtoai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
