{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1781f30",
   "metadata": {},
   "source": [
    "Import Statments and Parameters:\n",
    "\n",
    "--------------------------------------------------------------------\n",
    "\n",
    "IMPORTANT: Run these pip installs,\n",
    "\n",
    "If using miniconda: conda create --name project_env python=3.11\n",
    "\n",
    "pip install tensorflow keras numpy pandas requests Pillow scikit-learn matplotlib opencv-python jupyter ipykernel\n",
    "\n",
    "Download dataset here: https://iris.di.ubi.pt/ubipr.html (Original version, Change DATASET_PATH accordingly)\n",
    "\n",
    "--------------------------------------------------------------------\n",
    "\n",
    "IMG_SIZE - All Images scaled to this size.\n",
    "\n",
    "BATCH_SIZE - Groups pairs of images for siamese training.\n",
    "\n",
    "EPOCHS - number of epochs in training.\n",
    "\n",
    "DATASET_PATH - Path to dataset folder.\n",
    "\n",
    "STEPS_PER_EPOCH - Steps per each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267cd2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, Model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "\n",
    "# -----------------------------\n",
    "# PARAMETERS\n",
    "# -----------------------------\n",
    "IMG_SIZE = (105, 105)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4  # increase for better results\n",
    "\n",
    "# Load DATASET_PATH from dataset_path.txt (expected in the notebook working directory)\n",
    "dataset_txt = os.path.join(os.getcwd(), \"dataset_path.txt\")\n",
    "if os.path.exists(dataset_txt):\n",
    "    with open(dataset_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        DATASET_PATH = f.read().strip()\n",
    "    if not DATASET_PATH:\n",
    "        raise ValueError(\"dataset_path.txt is empty. Put the dataset path inside the file.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"dataset_path.txt not found at {dataset_txt}. Create it with the dataset path.\")\n",
    "STEPS_PER_EPOCH = 50\n",
    "# -----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ddb79",
   "metadata": {},
   "source": [
    "Image Preparation Definitions:\n",
    "\n",
    "load_images_by_filenames(dataset_path, img_size = IMG_SIZE) - Loads images from dataset_path scaled to IMG_SIZE, then splits into groups by person. Outputs dictionary where each person has a list of images.\n",
    "\n",
    "siamese_batch_generator(images_dict, batch_size=BATCH_SIZE) - Chooses a case (same person, different person), if same person, get 2 images of them label 1, otherwise get 2 different people images and label 0. This is done so dynamically as training is happening, so as to avoid large memory usage.\n",
    "\n",
    "make_tf_dataset(images_dict, batch_size=BATCH_SIZE) - Creates tensor flow dataset for image_dict for batch_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2ff735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# UTILITY FUNCTIONS\n",
    "# -----------------------------\n",
    "def load_images_by_filename(dataset_path, img_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Load images and group them by person_id extracted from filename (e.g., C1_S1).\n",
    "    \"\"\"\n",
    "    images_dict = {}\n",
    "    for img_name in os.listdir(dataset_path):\n",
    "        if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            person_id = \"_\".join(img_name.split(\"_\")[:2])  # e.g., C1_S1\n",
    "            img_path = os.path.join(dataset_path, img_name)\n",
    "            img = image.load_img(img_path, target_size=img_size)\n",
    "            img = image.img_to_array(img) / 255.0\n",
    "            if person_id not in images_dict:\n",
    "                images_dict[person_id] = []\n",
    "            images_dict[person_id].append(img)\n",
    "    if not images_dict:\n",
    "        raise ValueError(\"No images found in dataset. Check DATASET_PATH and file names.\")\n",
    "    return images_dict\n",
    "\n",
    "def siamese_batch_generator(images_dict, batch_size=BATCH_SIZE): \n",
    "    \"\"\" \n",
    "    Generate batches of pairs for Siamese network training. \n",
    "    Does so dynamically to avoid large memory usage. \n",
    "    Adds tiny noise to duplicate images to avoid exact zeros. \n",
    "    \"\"\" \n",
    "    person_ids = list(images_dict.keys()) \n",
    "    if not person_ids: \n",
    "        raise ValueError(\"No persons found in images_dict.\") \n",
    "    while True: \n",
    "        X1 = np.zeros((batch_size, *IMG_SIZE, 3), dtype=np.float32) \n",
    "        X2 = np.zeros((batch_size, *IMG_SIZE, 3), dtype=np.float32) \n",
    "        y = np.zeros((batch_size,), dtype=np.float32) \n",
    "        for i in range(batch_size): \n",
    "            if random.random() < 0.5: # same class \n",
    "                person = random.choice(person_ids) \n",
    "                imgs = images_dict[person] \n",
    "                if len(imgs) < 2: \n",
    "                    img1 = imgs[0] + np.random.normal(0, 1e-3, size=imgs[0].shape) \n",
    "                    img2 = imgs[0] + np.random.normal(0, 1e-3, size=imgs[0].shape) \n",
    "                else: \n",
    "                    img1, img2 = random.sample(imgs, 2) \n",
    "                label = 1 \n",
    "            else: # different class \n",
    "                if len(person_ids) > 1: \n",
    "                    person1, person2 = random.sample(person_ids, 2) \n",
    "                    img1 = random.choice(images_dict[person1]) \n",
    "                    img2 = random.choice(images_dict[person2]) \n",
    "                    label = 0 \n",
    "                else: # Only one person, duplicate image with tiny noise \n",
    "                    imgs = images_dict[person_ids[0]] \n",
    "                    img1 = imgs[0] + np.random.normal(0, 1e-3, size=imgs[0].shape) \n",
    "                    img2 = imgs[0] + np.random.normal(0, 1e-3, size=imgs[0].shape) \n",
    "                    label = 0.1 \n",
    "            X1[i] = img1 \n",
    "            X2[i] = img2 \n",
    "            y[i] = label \n",
    "        yield (X1, X2), y\n",
    "\n",
    "\n",
    "def make_tf_dataset(images_dict, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Wrap the generator in a tf.data.Dataset with proper output_signature.\n",
    "    \"\"\"\n",
    "    def gen():\n",
    "        for batch in siamese_batch_generator(images_dict, batch_size):\n",
    "            yield batch\n",
    "    \n",
    "    output_signature = (\n",
    "        (\n",
    "            tf.TensorSpec(shape=(batch_size, *IMG_SIZE, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(batch_size, *IMG_SIZE, 3), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9c3c3",
   "metadata": {},
   "source": [
    "Model Definition:\n",
    "\n",
    "create_base_cnn(input_shape=(*IMG_SIZE, 3)) - Creates CNN for model to extract features.\n",
    "\n",
    "euclidean_distance(vects) - Computes euclidean distance between 2 feature vectors to calculate similarity.\n",
    "\n",
    "create_siamese_model(input_shape=(*IMG_SIZE, 3)) - Creates siamese model which takes 2 input images, extracts their features, and outputs feature euclidean distance.\n",
    "\n",
    "contrastive_loss(y_true, y_pred, margin=1.0) - \n",
    "    Defines a loss metric with 2 goals:\n",
    "    1. When images are of the same person, the loss is small.\n",
    "    2. When images are of different people, the loss is large.\n",
    "    This loss function penalizes the model when either of the above goals are not achieved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2fbcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# SIAMESE NETWORK MODEL\n",
    "# -----------------------------\n",
    "def create_base_cnn(input_shape=(*IMG_SIZE, 3)):\n",
    "    \"\"\"\n",
    "    Base CNN to extract features from each image.\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, (7,7), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, (4,4), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(256, (4,4), activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(4096, activation='sigmoid')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"\n",
    "    Calculatees similarity in features of 2 images in terms of euclidean distance.\n",
    "    \"\"\"\n",
    "    x, y = vects\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def create_siamese_model(input_shape=(*IMG_SIZE, 3)):\n",
    "    \"\"\"\n",
    "    Creates a model that takes 2 input images, extracts their features, and outputs the distance between them.\n",
    "    \"\"\"\n",
    "    base_model = create_base_cnn(input_shape)\n",
    "    input_a = layers.Input(shape=input_shape)\n",
    "    input_b = layers.Input(shape=input_shape)\n",
    "    feat_a = base_model(input_a)\n",
    "    feat_b = base_model(input_b)\n",
    "    distance = layers.Lambda(euclidean_distance)([feat_a, feat_b])\n",
    "    model = Model([input_a, input_b], distance)\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# CONTRASTIVE LOSS\n",
    "# -----------------------------\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    \"\"\"\n",
    "    Defines a loss metric with 2 goals:\n",
    "    1. Distance small for same person.\n",
    "    2. Distance large for different people.\n",
    "    This loss function penalizes the model when either of the above goals are not achieved.\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "# -----------------------------\n",
    "# ACCURACY METRIC\n",
    "# -----------------------------\n",
    "@keras.saving.register_keras_serializable()\n",
    "def siamese_accuracy(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Definition of accuracy for siamese model.\n",
    "    \"\"\"\n",
    "    y_pred_binary = tf.cast(y_pred < threshold, tf.float32)\n",
    "    return tf.keras.metrics.binary_accuracy(y_true, y_pred_binary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713db7ea",
   "metadata": {},
   "source": [
    "Image Preparation: Generates tensorflow dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580c0397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C100_S1 15\n",
      "C100_S2 15\n",
      "C101_S1 15\n",
      "C101_S2 14\n",
      "C102_S1 15\n",
      "C102_S2 15\n",
      "C103_S1 15\n",
      "C103_S2 15\n",
      "C104_S1 15\n",
      "C104_S2 15\n",
      "C105_S1 15\n",
      "C105_S2 15\n",
      "C106_S1 15\n",
      "C106_S2 15\n",
      "C107_S1 15\n",
      "C107_S2 15\n",
      "C108_S1 15\n",
      "C108_S2 15\n",
      "C109_S1 15\n",
      "C109_S2 15\n",
      "C10_S1 15\n",
      "C10_S2 15\n",
      "C110_S1 15\n",
      "C110_S2 15\n",
      "C111_S1 15\n",
      "C111_S2 15\n",
      "C112_S1 15\n",
      "C112_S2 15\n",
      "C113_S1 15\n",
      "C114_S1 15\n",
      "C115_S1 15\n",
      "C116_S1 15\n",
      "C117_S1 15\n",
      "C118_S1 15\n",
      "C119_S1 15\n",
      "C11_S1 15\n",
      "C120_S1 15\n",
      "C121_S1 15\n",
      "C121_S2 15\n",
      "C122_S1 15\n",
      "C122_S2 15\n",
      "C123_S1 15\n",
      "C124_S1 15\n",
      "C125_S1 15\n",
      "C125_S2 15\n",
      "C126_S1 15\n",
      "C126_S2 15\n",
      "C127_S1 15\n",
      "C127_S2 15\n",
      "C128_S1 15\n",
      "C128_S2 15\n",
      "C129_S1 15\n",
      "C12_S1 15\n",
      "C130_S1 15\n",
      "C131_S1 15\n",
      "C132_S1 15\n",
      "C133_S1 15\n",
      "C134_S1 15\n",
      "C135_S1 15\n",
      "C136_S1 15\n",
      "C137_S1 15\n",
      "C138_S1 15\n",
      "C139_S1 15\n",
      "C13_S1 15\n",
      "C140_S1 15\n",
      "C141_S1 15\n",
      "C142_S1 15\n",
      "C143_S1 15\n",
      "C144_S1 15\n",
      "C145_S1 15\n",
      "C146_S1 15\n",
      "C147_S1 15\n",
      "C147_S2 15\n",
      "C148_S1 15\n",
      "C148_S2 15\n",
      "C149_S1 15\n",
      "C149_S2 15\n",
      "C14_S1 15\n",
      "C150_S1 15\n",
      "C150_S2 15\n",
      "C151_S1 15\n",
      "C151_S2 15\n",
      "C152_S1 15\n",
      "C152_S2 15\n",
      "C153_S1 15\n",
      "C154_S1 15\n",
      "C155_S1 15\n",
      "C156_S1 15\n",
      "C157_S1 15\n",
      "C158_S1 15\n",
      "C159_S1 15\n",
      "C15_S1 15\n",
      "C160_S1 15\n",
      "C161_S1 15\n",
      "C162_S1 15\n",
      "C163_S1 15\n",
      "C164_S1 15\n",
      "C165_S1 15\n",
      "C165_S2 15\n",
      "C166_S1 15\n",
      "C166_S2 15\n",
      "C167_S1 15\n",
      "C168_S1 15\n",
      "C169_S1 15\n",
      "C16_S1 15\n",
      "C170_S1 15\n",
      "C171_S1 15\n",
      "C171_S2 15\n",
      "C172_S1 15\n",
      "C172_S2 15\n",
      "C173_S1 15\n",
      "C174_S1 15\n",
      "C175_S1 15\n",
      "C175_S2 15\n",
      "C176_S1 15\n",
      "C176_S2 15\n",
      "C177_S1 15\n",
      "C177_S2 15\n",
      "C178_S1 15\n",
      "C178_S2 15\n",
      "C179_S1 15\n",
      "C17_S1 15\n",
      "C180_S1 15\n",
      "C181_S1 15\n",
      "C182_S1 15\n",
      "C183_S1 15\n",
      "C183_S2 15\n",
      "C184_S1 15\n",
      "C184_S2 15\n",
      "C185_S1 15\n",
      "C185_S2 15\n",
      "C186_S1 15\n",
      "C186_S2 15\n",
      "C187_S1 15\n",
      "C187_S2 15\n",
      "C188_S1 15\n",
      "C188_S2 15\n",
      "C189_S1 15\n",
      "C189_S2 15\n",
      "C18_S1 15\n",
      "C190_S1 15\n",
      "C190_S2 15\n",
      "C191_S1 15\n",
      "C192_S1 15\n",
      "C193_S1 15\n",
      "C194_S1 15\n",
      "C195_S1 15\n",
      "C196_S1 15\n",
      "C197_S1 15\n",
      "C198_S1 14\n",
      "C199_S1 15\n",
      "C199_S2 15\n",
      "C19_S1 15\n",
      "C1_S1 15\n",
      "C1_S2 15\n",
      "C200_S1 15\n",
      "C200_S2 15\n",
      "C201_S1 15\n",
      "C201_S2 15\n",
      "C202_S1 15\n",
      "C202_S2 15\n",
      "C203_S1 15\n",
      "C204_S1 15\n",
      "C205_S1 15\n",
      "C205_S2 15\n",
      "C206_S1 15\n",
      "C206_S2 15\n",
      "C207_S1 15\n",
      "C208_S1 15\n",
      "C209_S1 15\n",
      "C20_S1 15\n",
      "C210_S1 15\n",
      "C211_S1 15\n",
      "C212_S1 15\n",
      "C213_S1 15\n",
      "C214_S1 15\n",
      "C215_S1 15\n",
      "C216_S1 15\n",
      "C217_S1 15\n",
      "C218_S1 15\n",
      "C219_S1 15\n",
      "C21_S1 15\n",
      "C220_S1 15\n",
      "C221_S1 15\n",
      "C222_S1 15\n",
      "C223_S1 15\n",
      "C224_S1 15\n",
      "C225_S1 15\n",
      "C225_S2 15\n",
      "C226_S1 15\n",
      "C226_S2 15\n",
      "C227_S1 15\n",
      "C227_S2 15\n",
      "C228_S1 15\n",
      "C228_S2 15\n",
      "C229_S1 15\n",
      "C22_S1 15\n",
      "C230_S1 15\n",
      "C231_S1 15\n",
      "C232_S1 15\n",
      "C233_S1 15\n",
      "C233_S2 15\n",
      "C234_S1 15\n",
      "C234_S2 15\n",
      "C235_S1 14\n",
      "C235_S2 15\n",
      "C236_S1 15\n",
      "C236_S2 15\n",
      "C237_S1 15\n",
      "C237_S2 15\n",
      "C238_S1 15\n",
      "C238_S2 15\n",
      "C239_S1 15\n",
      "C239_S2 15\n",
      "C23_S1 15\n",
      "C240_S1 15\n",
      "C240_S2 15\n",
      "C241_S1 15\n",
      "C242_S1 15\n",
      "C243_S1 15\n",
      "C243_S2 15\n",
      "C244_S1 15\n",
      "C244_S2 15\n",
      "C245_S1 15\n",
      "C245_S2 15\n",
      "C246_S1 15\n",
      "C246_S2 15\n",
      "C247_S1 15\n",
      "C247_S2 15\n",
      "C248_S1 15\n",
      "C248_S2 15\n",
      "C249_S1 15\n",
      "C24_S1 15\n",
      "C250_S1 15\n",
      "C251_S1 15\n",
      "C252_S1 15\n",
      "C253_S1 15\n",
      "C253_S2 15\n",
      "C254_S1 15\n",
      "C254_S2 15\n",
      "C255_S1 15\n",
      "C255_S2 15\n",
      "C256_S1 15\n",
      "C256_S2 15\n",
      "C257_S1 15\n",
      "C257_S2 15\n",
      "C258_S1 15\n",
      "C258_S2 15\n",
      "C259_S1 15\n",
      "C259_S2 15\n",
      "C25_S1 15\n",
      "C260_S1 15\n",
      "C260_S2 15\n",
      "C261_S1 15\n",
      "C261_S2 15\n",
      "C262_S1 15\n",
      "C262_S2 15\n",
      "C263_S1 15\n",
      "C264_S1 15\n",
      "C265_S1 15\n",
      "C266_S1 15\n",
      "C267_S1 15\n",
      "C268_S1 15\n",
      "C269_S1 15\n",
      "C26_S1 14\n",
      "C270_S1 15\n",
      "C271_S1 14\n",
      "C272_S1 15\n",
      "C273_S1 15\n",
      "C274_S1 15\n",
      "C275_S1 15\n",
      "C276_S1 15\n",
      "C277_S1 15\n",
      "C278_S1 15\n",
      "C279_S1 15\n",
      "C27_S1 15\n",
      "C280_S1 15\n",
      "C281_S1 15\n",
      "C282_S1 15\n",
      "C283_S1 15\n",
      "C284_S1 15\n",
      "C285_S1 15\n",
      "C286_S1 15\n",
      "C287_S1 15\n",
      "C288_S1 15\n",
      "C289_S1 15\n",
      "C28_S1 15\n",
      "C290_S1 15\n",
      "C291_S1 15\n",
      "C292_S1 15\n",
      "C293_S1 15\n",
      "C294_S1 15\n",
      "C295_S1 15\n",
      "C296_S1 15\n",
      "C297_S1 15\n",
      "C297_S2 15\n",
      "C298_S1 15\n",
      "C298_S2 15\n",
      "C299_S1 15\n",
      "C29_S1 15\n",
      "C2_S1 15\n",
      "C2_S2 15\n",
      "C300_S1 15\n",
      "C301_S1 15\n",
      "C302_S1 15\n",
      "C303_S1 15\n",
      "C304_S1 15\n",
      "C305_S1 15\n",
      "C306_S1 15\n",
      "C307_S1 15\n",
      "C308_S1 15\n",
      "C309_S1 15\n",
      "C309_S2 15\n",
      "C30_S1 15\n",
      "C310_S1 15\n",
      "C310_S2 15\n",
      "C311_S1 15\n",
      "C312_S1 15\n",
      "C313_S1 15\n",
      "C313_S2 15\n",
      "C314_S1 15\n",
      "C314_S2 15\n",
      "C315_S1 15\n",
      "C316_S1 15\n",
      "C317_S1 15\n",
      "C317_S2 15\n",
      "C318_S1 15\n",
      "C318_S2 15\n",
      "C319_S1 15\n",
      "C319_S2 15\n",
      "C31_S1 15\n",
      "C320_S1 15\n",
      "C320_S2 15\n",
      "C321_S1 14\n",
      "C322_S1 14\n",
      "C323_S1 15\n",
      "C323_S2 15\n",
      "C324_S1 15\n",
      "C324_S2 15\n",
      "C325_S1 15\n",
      "C325_S2 14\n",
      "C326_S1 15\n",
      "C326_S2 15\n",
      "C327_S1 15\n",
      "C328_S1 15\n",
      "C329_S1 15\n",
      "C329_S2 15\n",
      "C32_S1 15\n",
      "C330_S1 15\n",
      "C330_S2 15\n",
      "C331_S1 15\n",
      "C331_S2 15\n",
      "C332_S1 15\n",
      "C332_S2 15\n",
      "C333_S1 15\n",
      "C334_S1 15\n",
      "C335_S1 15\n",
      "C335_S2 15\n",
      "C336_S1 15\n",
      "C336_S2 15\n",
      "C337_S1 15\n",
      "C338_S1 15\n",
      "C339_S1 15\n",
      "C33_S1 15\n",
      "C340_S1 15\n",
      "C341_S1 15\n",
      "C342_S1 15\n",
      "C343_S1 15\n",
      "C343_S2 15\n",
      "C344_S1 15\n",
      "C344_S2 15\n",
      "C345_S1 15\n",
      "C346_S1 15\n",
      "C347_S1 15\n",
      "C348_S1 15\n",
      "C349_S1 15\n",
      "C34_S1 15\n",
      "C350_S1 15\n",
      "C351_S1 15\n",
      "C352_S1 15\n",
      "C353_S1 15\n",
      "C354_S1 15\n",
      "C355_S1 15\n",
      "C356_S1 15\n",
      "C357_S1 15\n",
      "C357_S2 15\n",
      "C358_S1 15\n",
      "C358_S2 15\n",
      "C359_S1 15\n",
      "C35_S1 15\n",
      "C360_S1 15\n",
      "C361_S1 15\n",
      "C362_S1 15\n",
      "C363_S1 15\n",
      "C363_S2 15\n",
      "C364_S1 15\n",
      "C364_S2 15\n",
      "C365_S1 15\n",
      "C365_S2 15\n",
      "C366_S1 15\n",
      "C366_S2 15\n",
      "C367_S1 15\n",
      "C368_S1 15\n",
      "C369_S1 14\n",
      "C36_S1 15\n",
      "C370_S1 14\n",
      "C371_S1 15\n",
      "C371_S2 15\n",
      "C372_S1 15\n",
      "C372_S2 15\n",
      "C373_S1 15\n",
      "C374_S1 15\n",
      "C375_S1 15\n",
      "C376_S1 15\n",
      "C377_S1 15\n",
      "C377_S2 15\n",
      "C378_S1 15\n",
      "C378_S2 15\n",
      "C379_S1 15\n",
      "C379_S2 15\n",
      "C37_S1 15\n",
      "C380_S1 15\n",
      "C380_S2 15\n",
      "C381_S1 15\n",
      "C381_S2 15\n",
      "C382_S1 15\n",
      "C382_S2 15\n",
      "C383_S1 15\n",
      "C384_S1 15\n",
      "C385_S1 15\n",
      "C386_S1 15\n",
      "C387_S1 15\n",
      "C387_S2 15\n",
      "C388_S1 15\n",
      "C388_S2 15\n",
      "C389_S1 15\n",
      "C389_S2 15\n",
      "C38_S1 15\n",
      "C390_S1 15\n",
      "C390_S2 15\n",
      "C391_S1 15\n",
      "C391_S2 15\n",
      "C392_S1 15\n",
      "C392_S2 15\n",
      "C393_S1 15\n",
      "C394_S1 15\n",
      "C395_S1 15\n",
      "C396_S1 15\n",
      "C397_S1 15\n",
      "C398_S1 15\n",
      "C399_S1 15\n",
      "C39_S1 15\n",
      "C39_S2 1\n",
      "C3_S1 15\n",
      "C400_S1 15\n",
      "C401_S1 15\n",
      "C401_S2 15\n",
      "C402_S1 15\n",
      "C402_S2 15\n",
      "C403_S1 15\n",
      "C404_S1 15\n",
      "C405_S1 15\n",
      "C406_S1 15\n",
      "C40_S1 15\n",
      "C40_S2 1\n",
      "C411_S1 15\n",
      "C412_S1 15\n",
      "C413_S1 15\n",
      "C414_S1 15\n",
      "C415_S1 15\n",
      "C416_S1 15\n",
      "C417_S1 15\n",
      "C418_S1 15\n",
      "C419_S1 15\n",
      "C41_S1 15\n",
      "C420_S1 15\n",
      "C421_S1 15\n",
      "C421_S2 15\n",
      "C422_S1 15\n",
      "C422_S2 15\n",
      "C423_S1 15\n",
      "C424_S1 15\n",
      "C425_S1 15\n",
      "C425_S2 15\n",
      "C426_S1 15\n",
      "C426_S2 15\n",
      "C429_S1 15\n",
      "C42_S1 15\n",
      "C430_S1 15\n",
      "C431_S1 15\n",
      "C432_S1 15\n",
      "C433_S1 15\n",
      "C434_S1 15\n",
      "C434_S2 1\n",
      "C435_S1 15\n",
      "C436_S1 15\n",
      "C437_S1 15\n",
      "C438_S1 15\n",
      "C439_S1 15\n",
      "C43_S1 15\n",
      "C43_S2 14\n",
      "C440_S1 15\n",
      "C441_S1 15\n",
      "C441_S2 14\n",
      "C442_S1 15\n",
      "C442_S2 15\n",
      "C443_S1 15\n",
      "C443_S2 15\n",
      "C444_S1 15\n",
      "C444_S2 15\n",
      "C445_S1 15\n",
      "C445_S2 15\n",
      "C446_S1 15\n",
      "C446_S2 15\n",
      "C447_S1 14\n",
      "C448_S1 14\n",
      "C449_S1 15\n",
      "C449_S2 15\n",
      "C44_S1 15\n",
      "C44_S2 15\n",
      "C450_S1 12\n",
      "C450_S2 15\n",
      "C451_S1 15\n",
      "C451_S2 15\n",
      "C452_S1 14\n",
      "C452_S2 15\n",
      "C453_S1 13\n",
      "C454_S1 15\n",
      "C455_S1 13\n",
      "C456_S1 13\n",
      "C457_S1 15\n",
      "C458_S1 15\n",
      "C459_S1 12\n",
      "C459_S2 3\n",
      "C45_S1 15\n",
      "C45_S2 15\n",
      "C460_S1 11\n",
      "C460_S2 4\n",
      "C461_S1 15\n",
      "C462_S1 15\n",
      "C463_S1 14\n",
      "C464_S1 15\n",
      "C465_S1 15\n",
      "C465_S2 15\n",
      "C466_S1 15\n",
      "C466_S2 15\n",
      "C467_S1 15\n",
      "C467_S2 15\n",
      "C468_S1 15\n",
      "C468_S2 15\n",
      "C469_S1 15\n",
      "C46_S1 15\n",
      "C46_S2 15\n",
      "C470_S1 15\n",
      "C471_S1 14\n",
      "C472_S1 15\n",
      "C473_S1 15\n",
      "C474_S1 15\n",
      "C475_S1 15\n",
      "C476_S1 15\n",
      "C477_S1 15\n",
      "C478_S1 15\n",
      "C479_S1 15\n",
      "C47_S1 15\n",
      "C47_S2 15\n",
      "C480_S1 15\n",
      "C481_S1 15\n",
      "C482_S1 15\n",
      "C483_S1 15\n",
      "C483_S2 15\n",
      "C484_S1 15\n",
      "C484_S2 15\n",
      "C485_S1 15\n",
      "C486_S1 15\n",
      "C487_S1 15\n",
      "C488_S1 15\n",
      "C489_S1 15\n",
      "C48_S1 15\n",
      "C48_S2 15\n",
      "C490_S1 15\n",
      "C491_S1 15\n",
      "C492_S1 15\n",
      "C493_S1 15\n",
      "C494_S1 15\n",
      "C495_S1 15\n",
      "C496_S1 15\n",
      "C497_S1 15\n",
      "C498_S1 15\n",
      "C499_S1 15\n",
      "C49_S1 15\n",
      "C49_S2 15\n",
      "C4_S1 15\n",
      "C500_S1 15\n",
      "C501_S1 15\n",
      "C501_S2 15\n",
      "C502_S1 15\n",
      "C502_S2 15\n",
      "C503_S1 12\n",
      "C503_S2 15\n",
      "C504_S1 12\n",
      "C504_S2 15\n",
      "C505_S1 15\n",
      "C506_S1 15\n",
      "C507_S1 15\n",
      "C507_S2 15\n",
      "C508_S1 15\n",
      "C508_S2 15\n",
      "C509_S1 15\n",
      "C509_S2 15\n",
      "C50_S1 15\n",
      "C50_S2 15\n",
      "C510_S1 15\n",
      "C510_S2 15\n",
      "C511_S1 15\n",
      "C512_S1 15\n",
      "C513_S1 15\n",
      "C514_S1 15\n",
      "C515_S1 15\n",
      "C516_S1 15\n",
      "C51_S1 15\n",
      "C521_S2 1\n",
      "C52_S1 15\n",
      "C57_S1 15\n",
      "C58_S1 15\n",
      "C59_S1 15\n",
      "C5_S1 15\n",
      "C60_S1 14\n",
      "C61_S1 15\n",
      "C61_S2 15\n",
      "C62_S1 15\n",
      "C62_S2 15\n",
      "C63_S1 15\n",
      "C64_S1 15\n",
      "C65_S1 15\n",
      "C66_S1 15\n",
      "C67_S1 15\n",
      "C68_S1 15\n",
      "C69_S1 15\n",
      "C6_S1 15\n",
      "C70_S1 15\n",
      "C71_S1 15\n",
      "C72_S1 15\n",
      "C73_S1 15\n",
      "C73_S2 15\n",
      "C74_S1 15\n",
      "C74_S2 15\n",
      "C75_S1 15\n",
      "C76_S1 15\n",
      "C77_S1 15\n",
      "C78_S1 14\n",
      "C79_S1 15\n",
      "C7_S1 15\n",
      "C7_S2 15\n",
      "C80_S1 15\n",
      "C81_S1 15\n",
      "C82_S1 15\n",
      "C83_S1 15\n",
      "C84_S1 15\n",
      "C85_S1 15\n",
      "C85_S2 15\n",
      "C86_S1 15\n",
      "C86_S2 15\n",
      "C87_S1 15\n",
      "C88_S1 15\n",
      "C89_S1 15\n",
      "C89_S2 15\n",
      "C8_S1 15\n",
      "C8_S2 14\n",
      "C90_S1 15\n",
      "C90_S2 15\n",
      "C91_S1 15\n",
      "C91_S2 15\n",
      "C92_S1 15\n",
      "C92_S2 15\n",
      "C93_S1 15\n",
      "C93_S2 15\n",
      "C94_S1 15\n",
      "C94_S2 15\n",
      "C95_S1 15\n",
      "C95_S2 15\n",
      "C96_S1 15\n",
      "C96_S2 15\n",
      "C97_S1 15\n",
      "C98_S1 15\n",
      "C99_S1 15\n",
      "C99_S2 15\n",
      "C9_S1 15\n",
      "C9_S2 15\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# IMAGE PREPARATION\n",
    "# -----------------------------\n",
    "\n",
    "images_dict = load_images_by_filename(DATASET_PATH)\n",
    "for person, imgs in images_dict.items():\n",
    "    print(person, len(imgs))  # sanity check\n",
    "\n",
    "train_ds = make_tf_dataset(images_dict, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b00764",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab355fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Gunnar\\Files\\School_Stuff\\IntroToAi\\AiFinalProject\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">38,960,448</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m105\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m, \u001b[38;5;34m105\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)      │ \u001b[38;5;34m38,960,448\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ functional[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,960,448</span> (148.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m38,960,448\u001b[0m (148.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,960,448</span> (148.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,960,448\u001b[0m (148.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 790ms/step - loss: 0.1675 - siamese_accuracy: 0.7763\n",
      "Epoch 2/4\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 778ms/step - loss: 0.1076 - siamese_accuracy: 0.8863\n",
      "Epoch 3/4\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 780ms/step - loss: 0.0988 - siamese_accuracy: 0.9013\n",
      "Epoch 4/4\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 771ms/step - loss: 0.0907 - siamese_accuracy: 0.9081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fc57cd6270>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TRAINING\n",
    "# -----------------------------\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='loss',        # or 'val_loss' if using validation\n",
    "    patience=5,            # stop after 5 epochs with no improvement\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model = create_siamese_model()\n",
    "model.compile(loss=contrastive_loss,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              metrics=[siamese_accuracy])\n",
    "model.summary()\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e9360",
   "metadata": {},
   "source": [
    "Save: Saves model to siamese_eye_model.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a8dae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: siamese_eye_model.keras\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# SAVE MODEL\n",
    "# -----------------------------\n",
    "def save_siamese_model(model, save_path=\"siamese_eye_model\"):\n",
    "    \"\"\"\n",
    "    Saves the entire model (structure + weights + optimizer state)\n",
    "    to a .keras format (recommended).\n",
    "    \"\"\"\n",
    "    model.save(save_path)  # Creates a folder or .keras file\n",
    "    print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "# Call it after training:\n",
    "save_siamese_model(model, \"siamese_eye_model.keras\")  # Or a folder name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479700b",
   "metadata": {},
   "source": [
    "Load: Loads siamese_eye_model.keras.\n",
    "\n",
    "Note: Must define siamese network functions euclidean_distance() and contrastive_loss(). (Run model definition section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e6a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# LOAD MODEL\n",
    "# -----------------------------\n",
    "from keras.models import load_model\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "\n",
    "def load_siamese_model(save_path=\"siamese_eye_model.keras\"):\n",
    "    \"\"\"\n",
    "    Loads a saved Siamese model that uses custom contrastive loss.\n",
    "    \"\"\"\n",
    "    return load_model(save_path, custom_objects={\"contrastive_loss\": contrastive_loss})\n",
    "\n",
    "# Example:\n",
    "# loaded_model = load_siamese_model(\"siamese_eye_model.keras\")\n",
    "# print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99a0b8",
   "metadata": {},
   "source": [
    "Eye Identification Definition:\n",
    "\n",
    "load_gallery_embeddings(model, gallery_root, img_size=IMG_SIZE) - Returns a dictionary containing each person and a list of paths to their images.\n",
    "\n",
    "identify_eye(model, query_img_path, gallery_dict, margin=1.0, threshold=70.0) - Preprocesses query image, then runs it through the model to compare similarity with each image in the gallery, then computes similarity based on distance output by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23695407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = (105, 105)\n",
    "\n",
    "def load_gallery_embeddings(model, gallery_root, img_size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Loads all images from the gallery and computes embeddings.\n",
    "    Returns a dict: {identity: [image_paths]}\n",
    "    \"\"\"\n",
    "    gallery_dict = {}\n",
    "    for identity in os.listdir(gallery_root):\n",
    "        identity_path = os.path.join(gallery_root, identity)\n",
    "        if os.path.isdir(identity_path):\n",
    "            gallery_dict[identity] = [os.path.join(identity_path, f)\n",
    "                                      for f in os.listdir(identity_path)\n",
    "                                      if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "    return gallery_dict\n",
    "\n",
    "def identify_eye(model, query_img_path, gallery_dict, margin=1.0, threshold=70.0):\n",
    "    \"\"\"\n",
    "    Identify the identity of a query eye image.\n",
    "    - model: trained Siamese network\n",
    "    - query_img_path: path to the query image\n",
    "    - gallery_dict: {identity: [list of image paths]}\n",
    "    - margin: used for similarity scaling\n",
    "    - threshold: minimum similarity (%) to accept as known identity\n",
    "    \"\"\"\n",
    "    # Load and preprocess query image\n",
    "    query_img = image.load_img(query_img_path, target_size=IMG_SIZE)\n",
    "    query_img = image.img_to_array(query_img) / 255.0\n",
    "    query_img = np.expand_dims(query_img, axis=0)\n",
    "\n",
    "    identity_scores = {}\n",
    "\n",
    "    for identity, img_paths in gallery_dict.items():\n",
    "        similarities = []\n",
    "        for g_path in img_paths:\n",
    "            try:\n",
    "                g_img = image.load_img(g_path, target_size=IMG_SIZE)\n",
    "                g_img = image.img_to_array(g_img)/255.0\n",
    "                g_img = np.expand_dims(g_img, axis=0)\n",
    "                distance = float(model.predict([query_img, g_img], verbose=0)[0,0])\n",
    "                similarity = (1 - np.tanh(distance / margin)) * 100\n",
    "                similarities.append(similarity)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {g_path}: {e}\")\n",
    "        if similarities:\n",
    "            # Take maximum similarity among images for this identity\n",
    "            identity_scores[identity] = max(similarities)\n",
    "\n",
    "    if not identity_scores:\n",
    "        return \"No gallery images found\", 0.0\n",
    "\n",
    "    # Determine the best match\n",
    "    best_identity = max(identity_scores, key=identity_scores.get)\n",
    "    best_score = identity_scores[best_identity]\n",
    "\n",
    "    if best_score >= threshold:\n",
    "        return best_identity, best_score\n",
    "    else:\n",
    "        return \"Unknown\", best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff515d",
   "metadata": {},
   "source": [
    "Compare Eyes: Uses identify_eye() to compare query_image with images in gallery and outputs highest similarity image.\n",
    "\n",
    "------------------------------------------------------------------\n",
    "IMPORTANT:\n",
    "\n",
    "Run previous definition cells.\n",
    "\n",
    "Gallery should contain subfolders for each person containing images of their eyes.\n",
    "\n",
    "Subfolder name should be persons names/identifier.\n",
    "\n",
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1f3fd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified as: Chase with confidence 86.18%\n"
     ]
    }
   ],
   "source": [
    "model = load_siamese_model(\"siamese_eye_model.keras\")\n",
    "\n",
    "GALLERY_PATH = r\"C:\\Users\\Chase\\OneDrive\\Desktop\\Courses\\AI\\Project\\Eyes\"\n",
    "\n",
    "gallery_dict = load_gallery_embeddings(model, GALLERY_PATH)\n",
    "\n",
    "query_image = r\"C:\\Users\\Chase\\OneDrive\\Desktop\\Courses\\AI\\Project\\Test3.jpg\"\n",
    "\n",
    "\n",
    "identity, confidence = identify_eye(model, query_image, gallery_dict, margin=1.0, threshold=80.0)\n",
    "\n",
    "print(f\"Identified as: {identity} with confidence {confidence:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introtoai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
